# 樸素貝葉斯分類算法

在貝葉斯看來,用"經驗"進行"判斷"。
**第一輪的分級**:是已知類別而統計特徵,即某一特徵在該類中的出現概率,是把類別分解成特徵概率的過程。
**第二輪的還原:**是已知特徵而推測類類別,這裡將第一輪的結果用上,是把知道統計情況的特徵還原成某一類的過程。

| | |
|--|--|
|**算法**|樸素貝葉斯分類|
|**問題域**|有監督學習的分類問題|
|**輸入**|向量X:樣本的多種特徵信息值,<br>向量Y:對應的結果數值|
|**輸出**|預測模型,為線性函數|
|**用法**|輸入待預測的向量X,輸出預測結果向量Y|

樸素貝葉斯分類思路:
1) 統計樣本數據。需要統計先驗概率P(y)和似然度P(x|y)。
2) 根據待預測樣本所包含的特徵,對不同類分別進行後驗概率計算。如總的特徵有A,B,C三項,但待測樣本只包含A,C兩項,那y1後驗概率的計算方法就為P(y1)P(A|y1)P(B|y1)。
3) 比較y1,y2,...,yn的後驗概率,哪個的概率值最大就將其作為預測值輸出。

```python
#ch4-1.py
from sklearn.datasets import load_iris
#載入樸素貝葉斯分類算法
from sklearn.naive_bayes import MultinomialNB
X, y=load_iris(return_X_y=True)
#訓練模型
clf=MultinomialNB().fit(X,y)
#使用模型進行分數預測
print(clf.predict(X))
print('性能:',clf.score(X,y))
```

| | |
|--|--|
|**優點**|運用了統計學成熟的理論,可解釋性強,對於大規模數據集訓練效率較高|
|**缺點**|對數據樣本的特徵維度作了"彼此獨立"的假設,如果實際情況並非如此則可能導致預測偏差增加|
|**應用**|常用於垃圾郵分類,以及其他文本分類|